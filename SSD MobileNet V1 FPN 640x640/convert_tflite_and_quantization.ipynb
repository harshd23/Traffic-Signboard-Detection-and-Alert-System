{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tensorflow model to tflite:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF2 FORMAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = '../training_helper_directory'\n",
    "SCRIPT_PATH = WORKSPACE_PATH + '/scripts'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH + '/annotations'\n",
    "\n",
    "IMAGE_PATH = '../Dataset/\"PASCAL VOC Format Dataset\"'\n",
    "\n",
    "CUSTOM_MODEL_NAME = '/ssd_mobilenet_v1_fpn_640x640'\n",
    "MODEL_PATH = './models' + CUSTOM_MODEL_NAME\n",
    "CONFIG_PATH = MODEL_PATH + '/pipeline.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 14:49:11.628472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 14:49:12.511267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2137 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0402 14:49:14.455248 26756 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-04-02 14:49:17.344100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2137 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-04-02 14:49:17.383441: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "I0402 14:49:17.895804 26756 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-04-02 14:49:18.750385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2137 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0402 14:49:19.584482 26756 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n",
      "2024-04-02 14:49:20.321413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2137 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x00000153E2E8F700>, because it is not built.\n",
      "W0402 14:49:20.589515 26756 save_impl.py:71] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x00000153E2E8F700>, because it is not built.\n",
      "W0402 14:49:27.960592 26756 save.py:233] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 252). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: tflite-exported-models\\my-model\\saved_model\\assets\n",
      "I0402 14:49:31.182875 26756 builder_impl.py:779] Assets written to: tflite-exported-models\\my-model\\saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# TF2 FORMAT\n",
    "!python {SCRIPT_PATH + '/export_tflite_graph_tf2.py'} --pipeline_config_path {CONFIG_PATH} --trained_checkpoint_dir {MODEL_PATH + '/' } --output_directory tflite-exported-models\\my-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./tflite-exported-models/my-model/saved_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "converter.target_spec.experimental_supported_backends = \"GPU\"\n",
    "tflite_model = converter.convert()\n",
    "# tflite_fp16_model = converter.convert()\n",
    "\n",
    "#Save the model.\n",
    "with tf.io.gfile.GFile('ssd_mobilenet_model_quant_f16.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF1 Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_directory = 'tf1-exported-models\\my-model'\n",
    "\n",
    "# # Replace \"XXXX\" in the line below with the latest checkpoint in the /content/training directory\n",
    "# !python {SCRIPT_PATH + '/export_tflite_ssd_graph.py'} --trained_checkpoint_prefix {MODEL_PATH + '/' } --output_directory {output_directory} --pipeline_config_path {CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localpb = '/content/fine_tuned_model_lite/tflite_graph.pb'\n",
    "# tflite_file = '/content/fine_tuned_model_lite/detect.tflite'\n",
    "\n",
    "# # Can use Netron app to determine the input size: https://www.electronjs.org/apps/netron \n",
    "# input_size = [1, 300, 300, 3]\n",
    "# input_shape = {\"normalized_input_image_tensor\" : input_size}\n",
    "\n",
    "# print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
    "#     localpb, \n",
    "#     [\"normalized_input_image_tensor\"],\n",
    "#     [\"TFLite_Detection_PostProcess\",\"TFLite_Detection_PostProcess:1\",\"TFLite_Detection_PostProcess:2\",\"TFLite_Detection_PostProcess:3\"],\n",
    "#     input_shape\n",
    "# )\n",
    "\n",
    "# converter.allow_custom_ops = True\n",
    "# converter.inference_type = tf.uint8  # Tell converter to create a quantized model\n",
    "# converter.quantized_input_stats = {'normalized_input_image_tensor': (128, 128)}\n",
    "\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# open(tflite_file,'wb').write(tflite_model)\n",
    "\n",
    "# # Test that the conversion was successful - if any errors are generated, something went wrong!\n",
    "# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "# interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Get list of all images in train directory\n",
    "image_path = 'D:/Traffic SignBoard Recognition using Deep Learning/Dataset/PASCAL VOC Format Dataset/train'\n",
    "\n",
    "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
    "JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
    "png_file_list = glob.glob(image_path + '/*.png')\n",
    "bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
    "\n",
    "quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generator that provides a representative dataset\n",
    "# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "# First, get input details for model so we know how to preprocess images\n",
    "PATH_TO_MODEL = 'D:/Traffic SignBoard Recognition using Deep Learning/SSD MobileNet V1 FPN 640x640/ssd_mobilenet_model.tflite'\n",
    "interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "import random\n",
    "\n",
    "def representative_data_gen():\n",
    "  dataset_list = quant_image_list\n",
    "  quant_num = 300\n",
    "  for i in range(quant_num):\n",
    "    pick_me = random.choice(dataset_list)\n",
    "    image = tf.io.read_file(pick_me)\n",
    "\n",
    "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
    "      image = tf.io.decode_jpeg(image, channels=3)\n",
    "    elif pick_me.endswith('.png'):\n",
    "      image = tf.io.decode_png(image, channels=3)\n",
    "    elif pick_me.endswith('.bmp'):\n",
    "      image = tf.io.decode_bmp(image, channels=3)\n",
    "\n",
    "    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Initialize converter module\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('D:/Traffic SignBoard Recognition using Deep Learning/SSD MobileNet V1 FPN 640x640/tflite-exported-models/my-model/saved_model')\n",
    "\n",
    "# This enables quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# This sets the representative dataset for quantization\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# This ensures that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "# These set the input tensors to uint8 and output tensors to float32\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('./quant_model_lite/detect_quant.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to run custom TFLite model on test images to detect objects\n",
    "# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n",
    "\n",
    "# Import packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Define function for inferencing with TFLite model and displaying results\n",
    "\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
    "\n",
    "  # Grab filenames of all images in test folder\n",
    "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "  # Load the label map into memory\n",
    "  with open(lblpath, 'r') as f:\n",
    "      labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "  # Load the Tensorflow Lite model into memory\n",
    "  interpreter = Interpreter(model_path=modelpath)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Get model details\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "  height = input_details[0]['shape'][1]\n",
    "  width = input_details[0]['shape'][2]\n",
    "\n",
    "  float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "  input_mean = 127.5\n",
    "  input_std = 127.5\n",
    "\n",
    "  # Randomly select test images\n",
    "  images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "  # Loop over every image and perform detection\n",
    "  for image_path in images_to_test:\n",
    "\n",
    "      # Load image and resize to expected shape [1xHxWx3]\n",
    "      image = cv2.imread(image_path)\n",
    "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      imH, imW, _ = image.shape\n",
    "      image_resized = cv2.resize(image_rgb, (width, height))\n",
    "      input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "      if float_input:\n",
    "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "      # Perform the actual detection by running the model with the image as input\n",
    "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Retrieve detection results\n",
    "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n",
    "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n",
    "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n",
    "\n",
    "      detections = []\n",
    "\n",
    "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "      for i in range(len(scores)):\n",
    "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "              # Get bounding box coordinates and draw box\n",
    "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "\n",
    "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "              # Draw label\n",
    "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "\n",
    "      # All the results have been drawn on the image, now display the image\n",
    "      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(12,16))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "      # Save detection results in .txt files (for calculating mAP)\n",
    "      elif txt_only == True:\n",
    "\n",
    "        # Get filenames and paths\n",
    "        image_fn = os.path.basename(image_path)\n",
    "        base_fn, ext = os.path.splitext(image_fn)\n",
    "        txt_result_fn = base_fn +'.txt'\n",
    "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "        # Write results to text file\n",
    "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "        with open(txt_savepath,'w') as f:\n",
    "            for detection in detections:\n",
    "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n",
    "PATH_TO_IMAGES='../Dataset/PASCAL VOC Format Dataset/test'   #Path to test images folder\n",
    "PATH_TO_MODEL='./quant_model_lite/detect_quant.tflite'   #Path to .tflite model file\n",
    "PATH_TO_LABELS='../training_helper_directory/annotations/label_map.pbtxt'   #Path to labelmap.txt file\n",
    "min_conf_threshold=0.5   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "images_to_test = 10   #Number of images to run detection on\n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate quantized model mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove existing detection results first\n",
    "!rm /content/mAP/input/detection-results/*\n",
    "\n",
    "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
    "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
    "PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   # Path to quantized .tflite model file\n",
    "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
    "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
    "min_conf_threshold=0.1   # Confidence threshold\n",
    "\n",
    "# Use all the images in the test folder\n",
    "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
    "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
    "\n",
    "# Tell function to just save results and not display images\n",
    "txt_only = True\n",
    "\n",
    "# Run inferencing function!\n",
    "print('Starting inference on %d images...' % images_to_test)\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
    "print('Finished inferencing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "traffic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
